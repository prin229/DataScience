{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CID_27ISaRpJ"
   },
   "source": [
    "# Implementation of  LSH from scratch\n",
    "\n",
    "In this notebook,we will implement LSH from scratch and predict the labels of the test data. we then verify the correctness of the implementation using a \"grader\" function/cell which will match the implmentation.\n",
    "\n",
    "The grader fucntion would help to validate the correctness of the code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mR49rnr6ibOX"
   },
   "source": [
    "## Reading the data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1624178666885,
     "user": {
      "displayName": "Prince Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinV9FS4vrxCfZY3bcMX6KkV85uw7txafiGbIX-=s64",
      "userId": "08366713323217710669"
     },
     "user_tz": -330
    },
    "id": "oA1hSk2odHUy",
    "outputId": "8268c5b2-ece2-42c7-be08-785c689e24f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       tv future in the hands of viewers with home th...\n",
       "1       worldcom boss  left books alone  former worldc...\n",
       "2       tigers wary of farrell  gamble  leicester say ...\n",
       "3       yeading face newcastle in fa cup premiership s...\n",
       "4       ocean s twelve raids box office ocean s twelve...\n",
       "                              ...                        \n",
       "2220    cars pull down us retail figures us retail sal...\n",
       "2221    kilroy unveils immigration policy ex-chatshow ...\n",
       "2222    rem announce new glasgow concert us band rem h...\n",
       "2223    how political squabbles snowball it s become c...\n",
       "2224    souness delight at euro progress boss graeme s...\n",
       "Name: text, Length: 2225, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from csv file\n",
    "import pandas as pd\n",
    "data_path = 'lsh_data.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1624178672481,
     "user": {
      "displayName": "Prince Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinV9FS4vrxCfZY3bcMX6KkV85uw7txafiGbIX-=s64",
      "userId": "08366713323217710669"
     },
     "user_tz": -330
    },
    "id": "cKHb7v5edUiU",
    "outputId": "5cf99677-67dc-44f4-c264-231fe89567cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            509\n",
       "business         508\n",
       "politics         415\n",
       "tech             399\n",
       "entertainment    384\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Overiview\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcpy_Nrnig9V"
   },
   "source": [
    "### Creating Train and Test Datasets\n",
    "\n",
    "Note that the labels for test data will not be present in the dataset and hence they are mentioned as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624178676787,
     "user": {
      "displayName": "Prince Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinV9FS4vrxCfZY3bcMX6KkV85uw7txafiGbIX-=s64",
      "userId": "08366713323217710669"
     },
     "user_tz": -330
    },
    "id": "ncAK-oHFeKbS"
   },
   "outputs": [],
   "source": [
    "# The last 10 rows in the csv file are query points, so loading them into test data.\n",
    "# And loading the reamining points to train_data for which labels are given.\n",
    "\n",
    "train_data = df.iloc[:-10,:]\n",
    "test_data = df.iloc[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1624178686088,
     "user": {
      "displayName": "Prince Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinV9FS4vrxCfZY3bcMX6KkV85uw7txafiGbIX-=s64",
      "userId": "08366713323217710669"
     },
     "user_tz": -330
    },
    "id": "fc-SORtAfgqI",
    "outputId": "85a6f217-04ab-40d5-dd30-303baede6a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>NaN</td>\n",
       "      <td>junk e-mails on relentless rise spam traffic i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>top stars join us tsunami tv show brad pitt  r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rings of steel combat net attacks gambling is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2218</th>\n",
       "      <td>NaN</td>\n",
       "      <td>davies favours gloucester future wales hooker ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>NaN</td>\n",
       "      <td>beijingers fume over parking fees choking traf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>NaN</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>NaN</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>NaN</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>NaN</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text\n",
       "2215      NaN  junk e-mails on relentless rise spam traffic i...\n",
       "2216      NaN  top stars join us tsunami tv show brad pitt  r...\n",
       "2217      NaN  rings of steel combat net attacks gambling is ...\n",
       "2218      NaN  davies favours gloucester future wales hooker ...\n",
       "2219      NaN  beijingers fume over parking fees choking traf...\n",
       "2220      NaN  cars pull down us retail figures us retail sal...\n",
       "2221      NaN  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222      NaN  rem announce new glasgow concert us band rem h...\n",
       "2223      NaN  how political squabbles snowball it s become c...\n",
       "2224      NaN  souness delight at euro progress boss graeme s..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1jeyM0emKOw"
   },
   "source": [
    "## Custom Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU6mt5wq3Oyg"
   },
   "source": [
    "### Instructions:\n",
    "\n",
    "  1. Read in the train_data.\n",
    "  2. Vectorize train_data using sklearns built in tfidf vectorizer.\n",
    "  3. Ignore unigrams and make use of both **bigrams & trigrams** and also limit the **max features** to **4000** and **minimum document frequency** to **10**.\n",
    "  4. After the tfidf vectors are generated as mentioned above, next task is to generate random hyperplanes.\n",
    "  5. Generate **5 random hyperplanes**. And generate the hyperplanes using a random normal distribution with **mean zero and variance 1**. \n",
    "  6. i have set the **numpy random seed to zero**, we will use the  **np.random.normal** to generate the vectors for hyperplanes.\n",
    "\n",
    "  8. Once the hash table is generated now take in each of the query points from the test data.\n",
    "  9. Vectorize those query points using the same tfidf vectorizer as mentioned above.\n",
    "  10. Now use the hash function on this query point and fetch all the similar data points from the hashtable.\n",
    "  11. Use cosine similarity to compute **11-Nearest Neighbours** from the list of data points obtained in the above step.\n",
    "  12. Take a majority vote among the 11-Nearest Neighbours and predict the class label for the query point in the test data.\n",
    "  13. **In case of a tie** in the obtained labels from nearest neighbours, we can pick a label after sorting all the labels **alphabetically**(A-Z), i.e. for example labels starting with A would get more preference than labels starting with Z.\n",
    "  14. Repeat steps 9 to 13 for all the points in the test data and then finally return a list with all the predicted labels.\n",
    "  15. Note that there are a total of 10 data points in the test data so the final list so  return should be of length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the tfidf vectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising tfidf vectorizer with parameters mentioned above\n",
    "vectorizer = TfidfVectorizer( ngram_range=(2, 3),max_features=4000,min_df=10)  \n",
    "X_train = vectorizer.fit_transform(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining all the five planes p1,p2,p3,p4,p4\n",
    "trainmatrix=X_train.toarray() #converting sparsematrix to array\n",
    "np.random.seed(0)\n",
    "p1=np.random.normal(0,1,4000)\n",
    "p2=np.random.normal(0,1,4000)\n",
    "p3=np.random.normal(0,1,4000)\n",
    "p4=np.random.normal(0,1,4000)\n",
    "p5=np.random.normal(0,1,4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating hashtable for storing key as hashvalue generated using planes and value as list of indexes of the train data \n",
    "#indexes are stored  as value to get the labels in the next steps\n",
    "table={}\n",
    "labeltalbe={}\n",
    "for j,i in enumerate(trainmatrix):\n",
    "    sign1,sign2,sign3,sign4,sign5=0,0,0,0,0\n",
    "    if np.dot(i.T,p1) >=0:sign1=1\n",
    "    if np.dot(i.T,p2) >=0:sign2=1\n",
    "    if np.dot(i.T,p3) >=0:sign3=1\n",
    "    if np.dot(i.T,p4) >=0:sign4=1\n",
    "    if np.dot(i.T,p5) >=0:sign5=1\n",
    "    hashval=(sign1,sign2,sign3,sign4,sign5)\n",
    "    if hashval in table.keys():\n",
    "        table[hashval].append(j)\n",
    "    else:\n",
    "        table[hashval]=[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_val(x):\n",
    "    '''\n",
    "    Function for calculating magnitude of the vector\n",
    "    '''\n",
    "    sqr=sum([i**2 for i in x])\n",
    "    return sqr**0.5\n",
    "def cossm(x,y):\n",
    "    '''\n",
    "    Calculating Cosine-similarity\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        return np.dot(x.T,y)/(vect_val(x)*vect_val(y))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1624178721408,
     "user": {
      "displayName": "Prince Singh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GinV9FS4vrxCfZY3bcMX6KkV85uw7txafiGbIX-=s64",
      "userId": "08366713323217710669"
     },
     "user_tz": -330
    },
    "id": "YECivOCWfvGn"
   },
   "outputs": [],
   "source": [
    "# Please implement this fucntion and write your code wherever asked. Do NOT change the code snippets provided by us.\n",
    "\n",
    "def predictLabels (test_data):\n",
    "    \"\"\"\n",
    "    Given the test_data, return the labels for all the rows in the test data.\n",
    "    Follow the step by step instructions mentioned above.\n",
    "\n",
    "    \"\"\"\n",
    "    X_test=vectorizer.transform(test_data['text'])\n",
    "    testtable={}\n",
    "    testmatrix=X_test.toarray()\n",
    "    output=[]\n",
    "    for j,i in enumerate(testmatrix):\n",
    "        sign1,sign2,sign3,sign4,sign5=0,0,0,0,0\n",
    "        if np.dot(i.T,p1) >= 0:sign1=1\n",
    "        if np.dot(i.T,p2) >= 0:sign2=1\n",
    "        if np.dot(i.T,p3) >= 0:sign3=1\n",
    "        if np.dot(i.T,p4) >= 0:sign4=1\n",
    "        if np.dot(i.T,p5) >= 0:sign5=1\n",
    "        hashval=(sign1,sign2,sign3,sign4,sign5)\n",
    "        testtable[j]=hashval\n",
    "        \n",
    "    for j,i in enumerate(testmatrix):\n",
    "        neighbors=table[testtable[j]]\n",
    "        dis=[]\n",
    "        index=[]\n",
    "        for nb in neighbors:\n",
    "            dis.append(cossm(i,trainmatrix[nb]))\n",
    "            index.append(nb)\n",
    "        res=pd.DataFrame({'dis':dis,'index':index}).sort_values(by='dis',ascending=False).head(11).reset_index(drop=True)\n",
    "        classes=train_data['category'][res['index']].value_counts()\n",
    "        output.append(classes.index[0])\n",
    "    return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhnngvQkrnBB"
   },
   "source": [
    "## Grader Cell\n",
    "\n",
    "Executing the following Grader cell to verify the correctness the above implementation. This cell will print \"Success\" if implmentation of the predictLabels() is correct, else, it will print \"Failed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GX1sji2XrtmX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Success ******** Accuracy Achieved =  90 %\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "## GRADER CELL:\n",
    "# This cell will print \"Success\" if the implmentation of the predictLabels() is correct and the accuracy obtained is above 80%.\n",
    "# Else, it will print \"Failed\"\n",
    "###########################################\n",
    "import numpy as np\n",
    "\n",
    "# Predict the labels using the predictLabels() function\n",
    "Y_custom = np.array(predictLabels(test_data))\n",
    "\n",
    "# Reference grader array - DO NOT MODIFY IT\n",
    "Y_grader = np.array(['tech', 'entertainment', 'tech', 'sport', 'business', 'business', 'politics', 'entertainment', 'politics', 'sport'])\n",
    "\n",
    "# Calculating accuracy by comparing Y_grader and Y_custom\n",
    "accuracy = np.sum(Y_grader==Y_custom) * 10\n",
    "\n",
    "if accuracy >= 80:\n",
    "  print(\"******** Success ********\",\"Accuracy Achieved = \", accuracy,'%')\n",
    "else:\n",
    "  print(\"####### Failed #######\",\"Accuracy Achieved = \", accuracy,'%')\n",
    "  print(\"\\nY_grader = \\n\\n\", Y_grader)\n",
    "  print(\"\\n\",\"*\"*50)\n",
    "  print(\"\\nY_custom = \\n\\n\", Y_custom)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSH_from_Scratch_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
