# -*- coding: utf-8 -*-
"""spam_classifier_random_forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-qiTJyjTZwIPLBEFnoKWuH7OiNBaqBC_
"""

import numpy as np
import pandas as pd
import nltk
import string
import re
stopwords=nltk.corpus.stopwords.words('english')
ps=nltk.PorterStemmer()

nltk.download('stopwords')

def text_cleaning(data):
  clean_text="".join([char for char in data if char not in string.punctuation])
  tokens=re.split('\W+',clean_text)
  text=[ps.stem(word) for word in tokens if word not in stopwords] #use " ".join() when using ngrams
  return text

"""**Vectorizing the raw data(using N grams)**
it creates a document term matrix where counts still occupy the cell with combinations of adjacent words of length "n" in the text
"""

df=pd.read_csv('/content/drive/My Drive/SMSSpamCollection.tsv',sep='\t',header=None)
df.columns=['label','text']
df['cleaned_text']=df['text'].apply(lambda x:text_cleaning(x))
df['cleaned_text']
df['map_label']=df['label'].apply(lambda x:1 if x=='ham' else 0)
df['map_label'].head()

"""**Apply N-gram vectorizer using sklearn.features_extraction_text**"""

from sklearn.feature_extraction.text import CountVectorizer

ngram_vect=CountVectorizer(ngram_range=[2,2])
x_counts=ngram_vect.fit_transform(df['cleaned_text'])
print(x_counts.shape)
#ngram_vect.get_feature_names()

data=pd.DataFrame(x_counts.toarray())
data.columns=ngram_vect.get_feature_names()
data.head(2)

"""TF-IDF Equation w(i,j)=tf(i,j)*log(N/df(i))
tf(i,j)=number of times i occur in j divided by total number of terms in j.
df(i) number of documents containing i,N :total number of documents
documents is the text basically in case of confusion

**Apply tfidVectorizer using sklearn**
"""

df_1=pd.read_csv('/content/drive/My Drive/SMSSpamCollection.tsv',sep='\t',header=None)
df_1.columns=['label','text']
df_1['cleaned_text']=df['text'].apply(lambda x:text_cleaning(x))
df_1['cleaned_text'].head(2)

from sklearn.feature_extraction.text import  TfidfVectorizer

tfid_vect=TfidfVectorizer(analyzer=text_cleaning)
x_tfidf=tfid_vect.fit_transform(df_1['text'])
print(x_tfidf.shape)

new_df=pd.DataFrame(x_tfidf.toarray())
new_df.columns=tfid_vect.get_feature_names()
new_df.head()



"""**Feature Engineering**
creating useful features needed for model
"""

#creating feature which include the length of the text messages
df['text_length']=df['text'].apply(lambda x:len(x)-x.count(" ")) #count is used to count the number of white space in the string
df['text_length']

#creating a newfeature whcih include the number of punctuations in the mesages , this helps in detecting spam 
#or ham mesages
def count_punct(text):
  count=len([char for char in text if char  in string.punctuation])
  return round(count/(len(text)-text.count(" ")),3)*100
df['pun_count']=df['text'].apply(lambda x:count_punct(x))
df['pun_count'].head()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sb
# %matplotlib inline

bins=np.linspace(0,200,40)
plt.hist(df['text_length'],bins)
plt.title('text_length distribution')

bins=np.linspace(0,50,40)
plt.hist(df['pun_count'],bins)
plt.title('pun_length distribution')

"""**Transformation**
Process that alter each datapoint in a certain column in a systematic way (eg x*x,squareroot(x))
"""

#needed to do transformation beacase the pun_len column is left skewed
#performing box-cox transformation
for i in [1,2,3]:
  plt.hist(df['pun_count']**(1/i),bins=40)
  plt.title("transformation apply is {}".format(str(i)))
  plt.show()

"""**Machine Learning on data preprocessed**"""

#1/4 is transformation is best to take
dataframe=pd.concat([df['text_length'],df['pun_count'],new_df],axis=1)
dataframe.head()

#using random forest as a classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold,cross_val_score

rf=RandomForestClassifier(n_jobs=-1)
KFold(n_splits=5)
cross_val_score(rf,dataframe,df['map_label'],cv=5,scoring="accuracy",n_jobs=-1)

#random forest with holdout set
from sklearn.metrics import precision_recall_fscore_support as score
from sklearn.model_selection import train_test_split

#train and test split of a dataset with 20% data used for training
(x_train,x_text,y_train,y_test)=train_test_split(dataframe,df['map_label'],test_size=0.2)
rf=RandomForestClassifier(n_estimators=40,n_jobs=-1,max_depth=20)
rf_model=rf.fit(x_train,y_train)

#print this see to which are the important features that contribute in the model buliding
sorted(zip(rf_model.feature_importances_,x_train.columns),reverse=True)[0:10]

y_pred=rf_model.predict(x_text)
(precision,recall,fscore,support)=score(y_test,y_pred,pos_label=1,average='binary')
print("precision:{} /Recall:{} /Accuracy:{}".format(precision,recall,(y_pred==y_test).sum()/len(y_pred)))

