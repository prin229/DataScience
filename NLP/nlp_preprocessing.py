# -*- coding: utf-8 -*-
"""NLP_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mIQWmTenI0JDcZ7TeJug-OgQsAWNTWmy
"""

import numpy as np
import pandas as pd
import nltk
import string
import re
stopwords=nltk.corpus.stopwords.words('english')
ps=nltk.PorterStemmer()

nltk.download('stopwords')

string.punctuation

"""**python functions for cleaning the text , it includes removing stopwords,removing punctuation and performing stemming on the text**"""

def text_cleaning(data):
  clean_text="".join([char for char in data if char not in string.punctuation])
  tokens=re.split('\W+',clean_text)
  text=[ps.stem(word) for word in tokens if word not in stopwords]
  return text

df=pd.read_csv('/content/drive/My Drive/SMSSpamCollection.tsv',sep='\t',header=None)
df.columns=['label','text']
df['cleaned_text']=df['text'].apply(lambda x:text_cleaning(x))
df['cleaned_text']

"""**implementing count vectorizer using sklearn.feature_extrsction.text**

countvectorizer is basically used to count the frequency of unique words in the text,this basically gives the sparse numpymatix as output ,thats why it is converted to array before creating dataframe out of it
"""

from sklearn.feature_extraction.text import  CountVectorizer

countvectorizer=CountVectorizer(analyzer=text_cleaning)
counts_x=countvectorizer.fit_transform(df['text'])
counts_x.shape
#print(countvectorizer.get_feature_names())

data=pd.DataFrame(counts_x.toarray())
data.columns=[countvectorizer.get_feature_names()]
data.head()

